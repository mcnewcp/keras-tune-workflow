{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "This notebook works through an example workflow of tracking Keras experiments using [MLFlow](https://www.mlflow.org).  \n",
    "\n",
    "# Data\n",
    "\n",
    "The CA housing data will be used for this example, which is a simple regressiont task. It will be loaded from the `sklearn` data loader.  I'll split off 20% into a test set and an additional 20% into a validation set.  Finally, I'll standardize the data using `StandardScaler` ahead of modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.320900e+04</td>\n",
       "      <td>1.320900e+04</td>\n",
       "      <td>1.320900e+04</td>\n",
       "      <td>1.320900e+04</td>\n",
       "      <td>1.320900e+04</td>\n",
       "      <td>1.320900e+04</td>\n",
       "      <td>1.320900e+04</td>\n",
       "      <td>1.320900e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-4.203870e-15</td>\n",
       "      <td>-2.366862e-17</td>\n",
       "      <td>-6.491791e-15</td>\n",
       "      <td>-7.838617e-15</td>\n",
       "      <td>6.697144e-17</td>\n",
       "      <td>6.724040e-17</td>\n",
       "      <td>5.434665e-14</td>\n",
       "      <td>-3.987625e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000038e+00</td>\n",
       "      <td>1.000038e+00</td>\n",
       "      <td>1.000038e+00</td>\n",
       "      <td>1.000038e+00</td>\n",
       "      <td>1.000038e+00</td>\n",
       "      <td>1.000038e+00</td>\n",
       "      <td>1.000038e+00</td>\n",
       "      <td>1.000038e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.762384e+00</td>\n",
       "      <td>-2.211806e+00</td>\n",
       "      <td>-1.726104e+00</td>\n",
       "      <td>-1.146007e+00</td>\n",
       "      <td>-1.218236e+00</td>\n",
       "      <td>-1.615666e-01</td>\n",
       "      <td>-1.447234e+00</td>\n",
       "      <td>-2.352574e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.818430e-01</td>\n",
       "      <td>-8.539298e-01</td>\n",
       "      <td>-3.749430e-01</td>\n",
       "      <td>-1.777574e-01</td>\n",
       "      <td>-5.506318e-01</td>\n",
       "      <td>-5.476924e-02</td>\n",
       "      <td>-7.973662e-01</td>\n",
       "      <td>-1.112100e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-1.803598e-01</td>\n",
       "      <td>2.469603e-02</td>\n",
       "      <td>-7.949644e-02</td>\n",
       "      <td>-9.572724e-02</td>\n",
       "      <td>-2.241327e-01</td>\n",
       "      <td>-2.382653e-02</td>\n",
       "      <td>-6.477564e-01</td>\n",
       "      <td>5.319027e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.522506e-01</td>\n",
       "      <td>6.636966e-01</td>\n",
       "      <td>2.282902e-01</td>\n",
       "      <td>2.179204e-03</td>\n",
       "      <td>2.492909e-01</td>\n",
       "      <td>1.196224e-02</td>\n",
       "      <td>9.652246e-01</td>\n",
       "      <td>7.809940e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.843102e+00</td>\n",
       "      <td>1.861823e+00</td>\n",
       "      <td>5.130163e+01</td>\n",
       "      <td>6.303746e+01</td>\n",
       "      <td>2.943487e+01</td>\n",
       "      <td>9.682297e+01</td>\n",
       "      <td>2.947555e+00</td>\n",
       "      <td>2.624269e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\n",
       "count  1.320900e+04  1.320900e+04  1.320900e+04  1.320900e+04  1.320900e+04   \n",
       "mean  -4.203870e-15 -2.366862e-17 -6.491791e-15 -7.838617e-15  6.697144e-17   \n",
       "std    1.000038e+00  1.000038e+00  1.000038e+00  1.000038e+00  1.000038e+00   \n",
       "min   -1.762384e+00 -2.211806e+00 -1.726104e+00 -1.146007e+00 -1.218236e+00   \n",
       "25%   -6.818430e-01 -8.539298e-01 -3.749430e-01 -1.777574e-01 -5.506318e-01   \n",
       "50%   -1.803598e-01  2.469603e-02 -7.949644e-02 -9.572724e-02 -2.241327e-01   \n",
       "75%    4.522506e-01  6.636966e-01  2.282902e-01  2.179204e-03  2.492909e-01   \n",
       "max    5.843102e+00  1.861823e+00  5.130163e+01  6.303746e+01  2.943487e+01   \n",
       "\n",
       "           AveOccup      Latitude     Longitude  \n",
       "count  1.320900e+04  1.320900e+04  1.320900e+04  \n",
       "mean   6.724040e-17  5.434665e-14 -3.987625e-15  \n",
       "std    1.000038e+00  1.000038e+00  1.000038e+00  \n",
       "min   -1.615666e-01 -1.447234e+00 -2.352574e+00  \n",
       "25%   -5.476924e-02 -7.973662e-01 -1.112100e+00  \n",
       "50%   -2.382653e-02 -6.477564e-01  5.319027e-01  \n",
       "75%    1.196224e-02  9.652246e-01  7.809940e-01  \n",
       "max    9.682297e+01  2.947555e+00  2.624269e+00  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, test_size=0.2\n",
    ")\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "pd.DataFrame(X_train, columns=housing.feature_names).describe()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "The model used for this example will be a wide and deep network with the following characteristics:\n",
    "- a deep path with `n_hidden` hidden layers with `n_neurons` at each layer\n",
    "- a wide path connecting all inputs to the output\n",
    "- all layers are fully connected\n",
    "- two outpus:\n",
    "    - one from the deep path alone, fit to the target\n",
    "    - one from the concatenaded wide and deep paths, fit to the target\n",
    "\n",
    "This type of multi-output architecture is usually used as a regularization technique, but I'm simply employing it here so my example has more than one loss to simultaneously minimize.  This model is very similar to the regression example I used in my [intro to Keras](https://github.com/mcnewcp/book-geron-ml-sklearn-keras-tensorflow/blob/main/10-intro-ann-keras/10-intro-ann-keras.ipynb) notebook and from Chapter 10 of [Hands on ML](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/), so I won't explain the Keras code below.\n",
    "\n",
    "The model build code should be functionalized so that the hyperparameters are generalized in the build and compile steps for integration into hyperparameter tuning.  I'm pulling out the following hyperparameters for tuning:\n",
    "- `n_hidden`: number of hidden layers\n",
    "- `n_neurons`: number of neurons per layer\n",
    "- `activation`: activation funciton used in hidden layers\n",
    "\n",
    "*Note*: I'm not tuning learning rate here.  In general I think it's best practice to choose a sufficiently low learning rate, high number of epochs, and use early stopping.  The goal of this stage of hyperparameter tuning is to simply identify promising model candidates.  Once promising candidates have been identified, the learning rate will be fine tuned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 15:11:41.805855: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf version: 2.11.0 , keras version: 2.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print('tf version:', tf.__version__, \", keras version:\", keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=20, activation=\"relu\"):\n",
    "    inp = keras.layers.Input(shape=[8], name=\"input\")  # input layer\n",
    "    for layer in range(n_hidden):  # sequentially add hidden layers\n",
    "        if layer == 0:\n",
    "            hl = keras.layers.Dense(n_neurons, activation=activation)(inp)\n",
    "        else:\n",
    "            hl = keras.layers.Dense(n_neurons, activation=activation)(hl)\n",
    "    concat = keras.layers.Concatenate()([hl, inp])  # concat deep and wide paths\n",
    "    main_output = keras.layers.Dense(1, name=\"main_output\")(concat)  # combined output\n",
    "    aux_output = keras.layers.Dense(1, name=\"aux_output\")(hl)  # deep output\n",
    "    model = keras.Model(inputs=[inp], outputs=[main_output, aux_output])\n",
    "    model.compile(\n",
    "        loss=[\"mse\", \"mse\"],\n",
    "        loss_weights=[0.9, 0.1],  # weighting heavily towards main output\n",
    "        optimizer=keras.optimizers.SGD(learning_rate=5e-3),\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFlow Experiment Tracking\n",
    "\n",
    "[MLFLow](https://www.mlflow.org) is a full-featured end-to-end ML lifecycle management platform, but all I'll be using it for in this example is experiment tracking.  The [documentation on tracking](https://www.mlflow.org/docs/latest/tracking.html) is quite good and so I'm working primarily from that.  There is even an automatic logging submodule for Keras and Tensorflow, `mlflow.tensorflow.autolog()` which I will try out first. In addition, nearly anything can be logged manually, including categories of metrics, parameters, tags, and artifacts.  Artifacts can be nearly anything including plots or the modles themselves.\n",
    "\n",
    "## Auto-Logging\n",
    "\n",
    "First I'll give the auto logging a shot and see what it logs.  I've had issues with the auto logging submodule for scikit-learn because it simply logged too many parameters to be useful and after the experiment count reached a certain threshold, performance in the dashboard tool suffered.\n",
    "\n",
    "**Note**: the auto-logging submodule only works for tensorflow versions 2.3.0 - 2.11.0, which I had to specify manually with `pip` as the `conda` installer chose a version outside of that range.\n",
    "\n",
    "By default, on first execution, MLFlow creates the directory `./mlruns` on default and stores all experiment related information as individual files within.  Another option is to store the information as a SQLite database, or incorporate into Databricks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 15:13:56.531114: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 113ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/02/14 15:14:35 WARNING mlflow.tensorflow: Failed to infer model signature: Expected one of (pandas.DataFrame, numpy array, dictionary of (name -> numpy.ndarray), pyspark.sql.DataFrame) but got '<class 'list'>'\n",
      "2023/02/14 15:14:35 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/m8/0_prp1tj41s9n5xm0bfqp6wm0000gn/T/tmpzkj3b5iy/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/m8/0_prp1tj41s9n5xm0bfqp6wm0000gn/T/tmpzkj3b5iy/model/data/model/assets\n",
      "2023/02/14 15:14:48 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/Users/mcnewcp/code/keras-tune-workflow/.env/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/129 [==============================] - 0s 2ms/step - loss: 0.3087 - main_output_loss: 0.3016 - aux_output_loss: 0.3722\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment(experiment_name=\"auto-log\")\n",
    "mlflow.tensorflow.autolog()  # turn on auto logging\n",
    "\n",
    "model = build_model(n_hidden=2)\n",
    "\n",
    "with mlflow.start_run(run_name=\"auto-log-1\") as run:\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        [y_train, y_train],\n",
    "        epochs=250,\n",
    "        validation_data=(X_valid, [y_valid, y_valid]),\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "        ],\n",
    "        verbose=0,\n",
    "    )\n",
    "    total_loss, main_loss, aux_loss = model.evaluate([X_test], [y_test, y_test])\n",
    "    mlflow.log_metrics(\n",
    "        {  # manually log test losses\n",
    "            \"test_total\": total_loss,\n",
    "            \"test_main\": main_loss,\n",
    "            \"test_aux\": aux_loss,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Server\n",
    "\n",
    "To serve the exploration tool locally, you simply run the following in the command line:\n",
    "\n",
    "```zsh\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "By default, this will look for local logged files in `./mlruns` and it will launch on port 5000.  If you need to change the location use `--backend-store-uri` and if you need to specify the port use `-p`.\n",
    "\n",
    "The UI provides a simple table comparison of all runs within an experiment so you can quickly check the parameters used for each run and corresponding metrics to choose promising model candidates.  It looks like the auto logger logs everything I need, including other helpful information like early stopping results and learning curves for each loss (see below).  It looks like the best option is the one investigated above: use the auto logger with a couple additional manually logged metrics and/or tags where necessary. \n",
    "\n",
    "![MLFLow UI Screenshot](images/mlflow_ui_sn.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b06340917b32c233dd2dc2960a5f67050ebf8c214e1ba3d4b841fb841861705b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
